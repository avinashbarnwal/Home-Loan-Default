{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is needed for python 2.7 ; probably not for python 3\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train   = pd.read_pickle('application_train.p')\n",
    "bureau_1            = pd.read_pickle('bureau_1.p')\n",
    "bureau_2            = pd.read_pickle('bureau_2.p')\n",
    "bureau_3            = pd.read_pickle('bureau_3.p')\n",
    "bureau_4            = pd.read_pickle('bureau_4.p')\n",
    "prev_data           = pd.read_pickle('prev_data.p')\n",
    "pos_data            = pd.read_pickle(\"pos_data.p\")\n",
    "ins_data            = pd.read_pickle(\"ins_data.p\")\n",
    "cc_data             = pd.read_pickle(\"cc_data.p\")\n",
    "application_test    = pd.read_pickle('application_test.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.merge(application_train,bureau_1,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,bureau_2,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,bureau_3,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,bureau_4,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,prev_data,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,pos_data,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,ins_data,on = 'SK_ID_CURR',how='left')\n",
    "application_train = pd.merge(application_train,cc_data,on = 'SK_ID_CURR',how='left')\n",
    "\n",
    "\n",
    "application_test = pd.merge(application_test,bureau_1,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,bureau_2,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,bureau_3,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,bureau_4,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,prev_data,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,pos_data,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,ins_data,on = 'SK_ID_CURR',how='left')\n",
    "application_test = pd.merge(application_test,cc_data,on = 'SK_ID_CURR',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = application_train['TARGET']\n",
    "X_train = application_train.drop(['TARGET','SK_ID_CURR'], axis=1)\n",
    "dtrain  = xgb.DMatrix(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out any parameter you don't want to test\n",
    "def XGB_CV(\n",
    "          max_depth,\n",
    "         # n_estimators,    \n",
    "          gamma,\n",
    "          subsample,\n",
    "          colsample_bytree,\n",
    "          min_child_weight,\n",
    "          max_delta_step\n",
    "         ):\n",
    "\n",
    "    global AUCbest\n",
    "    global ITERbest\n",
    "\n",
    "#\n",
    "# Define all XGboost parameters\n",
    "#\n",
    "\n",
    "    paramt = {\n",
    "              'booster' : 'gbtree',\n",
    "              'max_depth' : int(max_depth),\n",
    "              #'n_estimators' : max(min(n_estimators, 50), 0), \n",
    "              'gamma' : gamma,\n",
    "              'eta' : 0.1,\n",
    "              'objective' : 'binary:logistic',\n",
    "              'nthread' : 4,\n",
    "              'silent' : True,\n",
    "              'eval_metric': 'logloss',\n",
    "              'subsample' : max(min(subsample, 1), 0),\n",
    "              'colsample_bytree' : max(min(colsample_bytree, 1), 0),\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'max_delta_step' : int(max_delta_step),\n",
    "              'seed' : 1001\n",
    "              }\n",
    "\n",
    "    folds = 5\n",
    "    cv_score = 0\n",
    "\n",
    "    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, paramt), file=log_file )\n",
    "    log_file.flush()\n",
    "\n",
    "    xgbc = xgb.cv(\n",
    "                    paramt,\n",
    "                    dtrain,\n",
    "                    num_boost_round = 20000,\n",
    "                    stratified = True,\n",
    "                    nfold = folds,\n",
    "#                    verbose_eval = 10,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    metrics = 'auc',\n",
    "                    show_stdv = True\n",
    "               )\n",
    "\n",
    "# This line would have been on top of this section\n",
    "#    with capture() as result:\n",
    "\n",
    "# After xgb.cv is done, this section puts its output into log file. Train and validation scores \n",
    "# are also extracted in this section. Note the \"diff\" part in the printout below, which is the \n",
    "# difference between the two scores. Large diff values may indicate that a particular set of \n",
    "# parameters is overfitting, especially if you check the CV portion of it in the log file and find \n",
    "# out that train scores were improving much faster than validation scores.\n",
    "\n",
    "#    print('', file=log_file)\n",
    "#    for line in result[1]:\n",
    "#        print(line, file=log_file)\n",
    "#    log_file.flush()\n",
    "\n",
    "    val_score   = xgbc['test-auc-mean'].iloc[-1]\n",
    "    train_score = xgbc['train-auc-mean'].iloc[-1]\n",
    "    print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),\n",
    "(val_score*2-1)) )\n",
    "    if ( val_score > AUCbest ):\n",
    "        AUCbest = val_score\n",
    "        ITERbest = len(xgbc)\n",
    "\n",
    "    return (val_score*2) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log file. If you repeat this run, new output will be added to it\n",
    "log_file = open('AUC-5fold-XGB-run-01-v1-full.log', 'a')\n",
    "AUCbest  = -1.\n",
    "ITERbest = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianOptimization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c8879bdf4aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m XGB_BO = BayesianOptimization(XGB_CV, {\n\u001b[0m\u001b[1;32m      2\u001b[0m                                      \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     \u001b[0;31m# 'n_estimators' : (50, 400),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      \u001b[0;34m'gamma'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      \u001b[0;34m'subsample'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BayesianOptimization' is not defined"
     ]
    }
   ],
   "source": [
    "XGB_BO = BayesianOptimization(XGB_CV, {\n",
    "                                     'max_depth': (2, 12),\n",
    "                                    # 'n_estimators' : (50, 400),    \n",
    "                                     'gamma': (0.001, 10.0),\n",
    "                                     'subsample': (0.4, 1.0),\n",
    "                                     'colsample_bytree' :(0.4, 1.0),\n",
    "                                     'min_child_weight': (0, 20),\n",
    "                                     'max_delta_step': (0,10)\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0400063c5497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_file' is not defined"
     ]
    }
   ],
   "source": [
    "print('-'*130)\n",
    "print('-'*130, file=log_file)\n",
    "log_file.flush()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    XGB_BO.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In my version of sklearn there are many warning thrown out by the GP portion of this code. This is set to prevent them from showing on screen.\n",
    "#If you have a special relationship with your computer and want to know everything it is saying back, you'd probably want to remove the two \"warnings\" lines and slide the XGB_BO line all the way left.\n",
    "#I am doing only 2 initial points, which along with 8 exploratory points above makes it 10 \"random\" parameter combinations. I'd say that 15-20 is usually adequate. For n_iter 25-50 is usually enough.\n",
    "#There are several commented out maximize lines that could be worth exploring. The exact combination of parameters determines exploitation vs. exploration. It is tough to know which would work better without actually trying, though in my hands exploitation with \"expected improvement\" usually works the best. That's what the XGB_BO.maximize line below is specifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Final Results\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'XGB_BO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ece9b7568d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final Results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Maximum XGBOOST value: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mXGB_BO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best XGBOOST parameters: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXGB_BO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGB_BO' is not defined"
     ]
    }
   ],
   "source": [
    "print('-'*130)\n",
    "print('Final Results')\n",
    "print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'])\n",
    "print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'])\n",
    "print('-'*130, file=log_file)\n",
    "print('Final Result:', file=log_file)\n",
    "print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'], file=log_file)\n",
    "print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'], file=log_file)\n",
    "log_file.flush()\n",
    "log_file.close()\n",
    "\n",
    "history_df = pd.DataFrame(XGB_BO.res['all']['params'])\n",
    "history_df2 = pd.DataFrame(XGB_BO.res['all']['values'])\n",
    "history_df = pd.concat((history_df, history_df2), axis=1)\n",
    "history_df.rename(columns = { 0 : 'gini'}, inplace=True)\n",
    "history_df['AUC'] = ( history_df['gini'] + 1 ) / 2\n",
    "history_df.to_csv('AUC-5fold-XGB-run-01-v1-grid.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
